{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce5f5f-3c92-4638-b516-4dfca4c92520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine RAG and Vector db (FAISS) with LLM model to extract relevant data from latest documents for a User Query\n",
    "# RAGVectorDBFaiss_Llama3_InsuranceClaim\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557e4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy torch transformers faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42921195-cb13-4b56-a5ac-965360ae42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hfg_token = \" \"\n",
    "login(token=hfg_token)\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = hfg_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a73b66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f690062d585a4723888c522fdf3bc47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Initialize the model and tokenizer for the LLM\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\" #'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08e228a-955a-4cf3-b257-ba7247856df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('documents.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS documents (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    text TEXT NOT NULL,\n",
    "    embedding TEXT NOT NULL\n",
    ")\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224c1680-c920-4a35-bcca-0bbbd198be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501503d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "# Sample health insurance documents (for demonstration)\n",
    "documents = [\n",
    "    \"Policy Holder: John Doe, Claim Amount: $5000, Diagnosis: Flu.\",\n",
    "    \"Policy Holder: Jane Smith, Claim Amount: $15000, Diagnosis: ADHD.\",\n",
    "    \"Policy Holder: Sam Brown, Claim Amount: $10000, Diagnosis: Ulcer.\",\n",
    "    \"Auto Insurance Claimant: Elliot, Claim Amount: $2,500, Paid: No\",\n",
    "    \"Auto Insurance Claimant: John, Claim Amount: $3,000, Paid: Partial\",\n",
    "    \"Auto Insurance Claimant: David, Claim Amount: $7,500, Paid: Yes\"\n",
    "]\n",
    "# Step 1: Create a FAISS index for vector retrieval\n",
    "embeddings = []\n",
    "\n",
    "for doc in documents:\n",
    "    # Encode the document using the LLM\n",
    "    inputs = tokenizer(doc, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    # Use the last hidden state as the embedding\n",
    "    embeddings.append(outputs.logits.mean(dim=1).detach().numpy())\n",
    "    json_embed = json.dumps(np.array(embeddings).tolist())\n",
    "    cursor.execute('INSERT INTO documents (text, embedding) VALUES (?, ?)', (doc, json_embed))\n",
    "conn.commit()\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)  # Add embeddings to the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f038bf6-fae5-4166-8741-8eca8eba1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM {'documents'} Limit 1;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print column names\n",
    "column_names = [description[0] for description in cursor.description]\n",
    "print(column_names)\n",
    "\n",
    "# Print all rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "['id', 'text', 'embedding']\n",
    "(1, 'The quick brown fox jumps over the lazy dog.', b'BM\\xa8\\xc2d\\xce\\xa8\\xc2\\x15\\x87\\xb1\\xc2J\\xa9\\xb3\\xc2m\\\n",
    "xa1\\xb0\\xc2x\\x83\\xb0\\xc2\\xda)\\xaa\\xc2\\x1c\\xa0\\xab\\xc2\\xc1\\xb6\\xa9\\xc2@\\x88\\xac\\xc2\\xfc\\x84\\xaf\\xc2\\xe8\\x88\\x9f\\\n",
    "xc2N\\xa7\\xa2\\xc2\\xb2\\xb0\\xa2\\xc25~\\xa7\\xc2\\x12\\x13\\xb1\\xc\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bbf58f-add4-4782-8ede-6ea7bf447b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 0: [ 4.063478   3.634199   5.0635753 ... -5.2163105 -5.216363  -5.216341 ]\n",
      "Vector 1: [ 4.0459576  3.3205242  4.744379  ... -5.6514874 -5.651537  -5.6515217]\n",
      "Vector 2: [ 4.16554    3.1222484  4.652902  ... -5.5586405 -5.558699  -5.558677 ]\n",
      "Vector 3: [ 5.640151   3.7278209  5.2984076 ... -5.414397  -5.414443  -5.4143996]\n",
      "Vector 4: [ 5.697008   3.4584913  5.0969944 ... -5.6340694 -5.6341114 -5.634085 ]\n",
      "Vector 5: [ 5.749886   3.707664   5.148497  ... -5.6200976 -5.620149  -5.6201205]\n"
     ]
    }
   ],
   "source": [
    "# To print all the vectors stored in the index\n",
    "for i in range(index.ntotal):\n",
    "    vector = index.reconstruct(i)\n",
    "    print(f\"Vector {i}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e657191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to retrieve similar documents\n",
    "def retrieve_similar_documents(query, top_k=2):\n",
    "    # Encode the query\n",
    "    document_inputs = tokenizer(query, return_tensors='pt')\n",
    "    query_output = model(**document_inputs)\n",
    "    query_embedding = query_output.logits.mean(dim=1).detach().numpy()\n",
    "\n",
    "    # Search for the closest documents in the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [documents[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da942cfe-d75a-420a-807d-c06968eadf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Documents: ['Auto Insurance Claimant: John, Claim Amount: $3,000, Paid: Partial', 'Policy Holder: Sam Brown, Claim Amount: $10000, Diagnosis: Ulcer.']\n"
     ]
    }
   ],
   "source": [
    "# Example query for fraud detection\n",
    "query = \"Claim Amount: $10000\"\n",
    "similar_docs = retrieve_similar_documents(query)\n",
    "print(\"Similar Documents:\", similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fd5b2a1-d024-47b8-a4eb-7646787241b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7feb4fe894f4786925e34551ad3f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give as output only the claim amount in the given text Auto Insurance Claimant: John, Claim Amount: $3,000, Paid: Partial Policy Holder: Sam Brown, Claim Amount: $10000, Diagnosis: Ulcer. The output should be: 3000 10000\n",
      "```\n",
      "import re\n",
      "\n",
      "text = \"Auto Insurance Claimant: John, Claim Amount: $3,000, Paid: Partial Policy Holder: Sam Brown, Claim Amount: $10000, Diagnosis: Ulcer\"\n",
      "print(re.findall(r'Claim Amount: \\$(\\d+)', text))\n",
      "```\n",
      "```\n",
      "['3000', '10000']\n",
      "```\n",
      "CLICK HERE to find out more related problems solutions.\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define your prompt\n",
    "prompt_text = \" \".join(similar_docs)\n",
    "prompt = \"Give as output only the claim amount in the given text\"\n",
    "\n",
    "# Generate text\n",
    "prompt_output = text_generator(f\"{prompt} {prompt_text}\", max_new_tokens=100)[0]['generated_text']\n",
    "\n",
    "# Print the generated text\n",
    "print(prompt_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67a22e5-ea17-4873-ab73-716693ecfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load embeddings from the database\n",
    "# cursor.execute('SELECT id, embedding FROM documents')\n",
    "# rows = cursor.fetchall()\n",
    "\n",
    "# # Prepare data for FAISS\n",
    "# ids = []\n",
    "# embeddings = []\n",
    "# for row in rows:\n",
    "#     ids.append(row[0])\n",
    "#     embeddings.append(np.array(row[1], dtype=np.unicode_))\n",
    "\n",
    "\n",
    "# embeddings = np.stack(embeddings)\n",
    "\n",
    "# # Create FAISS index\n",
    "# dimension = embeddings.shape[1]\n",
    "# index = faiss.IndexFlatL2(dimension)\n",
    "# index.add(embeddings)\n",
    "\n",
    "# # Save FAISS index\n",
    "# faiss.write_index(index, 'faiss_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a46389-ed65-4e57-8e64-2cb9e588e5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
